import { randomUUID } from 'crypto';
import { PDFExtractor } from './PDFExtractor';
import { DocumentChunker, CHUNKING_CONFIGS } from '../chunking/DocumentChunker';
import { AdaptiveChunker } from '../chunking/AdaptiveChunker';
import { VectorStore } from '../vector-store/VectorStore';
import { EnhancedVectorStore } from '../vector-store/EnhancedVectorStore';
import { OllamaClient } from '../llm/OllamaClient';
import { CitationExtractor } from '../analysis/CitationExtractor';
import { DocumentSummarizer, type SummarizerConfig } from '../analysis/DocumentSummarizer';
import type { PDFDocument, Citation } from '../../types/pdf-document';

export interface IndexingProgress {
  stage:
    | 'extracting'
    | 'analyzing'
    | 'citations'
    | 'summarizing'
    | 'chunking'
    | 'embedding'
    | 'similarities'
    | 'completed'
    | 'error';
  progress: number; // 0-100
  message: string;
  currentPage?: number;
  totalPages?: number;
  currentChunk?: number;
  totalChunks?: number;
}

export class PDFIndexer {
  private pdfExtractor: PDFExtractor;
  private chunker: DocumentChunker | AdaptiveChunker;
  private vectorStore: VectorStore | EnhancedVectorStore;
  private ollamaClient: OllamaClient;
  private citationExtractor: CitationExtractor;
  private documentSummarizer: DocumentSummarizer | null = null;
  private summarizerConfig: SummarizerConfig;
  private useAdaptiveChunking: boolean;

  constructor(
    vectorStore: VectorStore | EnhancedVectorStore,
    ollamaClient: OllamaClient,
    chunkingConfig: 'cpuOptimized' | 'standard' | 'large' = 'cpuOptimized',
    summarizerConfig?: SummarizerConfig,
    useAdaptiveChunking: boolean = false
  ) {
    this.pdfExtractor = new PDFExtractor();
    this.useAdaptiveChunking = useAdaptiveChunking;

    // Choose chunker based on configuration
    if (useAdaptiveChunking) {
      console.log('üìê Using AdaptiveChunker (structure-aware)');
      this.chunker = new AdaptiveChunker(CHUNKING_CONFIGS[chunkingConfig]);
    } else {
      console.log('üìê Using DocumentChunker (fixed-size)');
      this.chunker = new DocumentChunker(CHUNKING_CONFIGS[chunkingConfig]);
    }

    this.vectorStore = vectorStore;
    this.ollamaClient = ollamaClient;
    this.citationExtractor = new CitationExtractor();

    // Initialiser DocumentSummarizer si activ√©
    this.summarizerConfig = summarizerConfig || {
      enabled: false,
      method: 'extractive',
      maxLength: 250,
    };

    if (this.summarizerConfig.enabled) {
      this.documentSummarizer = new DocumentSummarizer(this.summarizerConfig, ollamaClient);
    }
  }

  /**
   * Indexe un PDF complet
   * @param filePath Chemin vers le fichier PDF
   * @param bibtexKey Cl√© BibTeX optionnelle pour lier √† la bibliographie
   * @param onProgress Callback pour la progression
   * @param bibliographyMetadata M√©tadonn√©es optionnelles provenant de la bibliographie (prioritaires sur l'extraction PDF)
   */
  async indexPDF(
    filePath: string,
    bibtexKey?: string,
    onProgress?: (progress: IndexingProgress) => void,
    bibliographyMetadata?: { title?: string; author?: string; year?: string }
  ): Promise<PDFDocument> {
    console.log('üîç [INDEXER] Starting PDF indexing...');
    console.log(`   File: ${filePath}`);
    console.log(`   BibtexKey: ${bibtexKey || 'none'}`);

    try {
      // 1. Extraire le texte + m√©tadonn√©es
      console.log('üîç [INDEXER] Step 1: Extracting text and metadata...');
      onProgress?.({
        stage: 'extracting',
        progress: 10,
        message: 'Extraction du texte PDF...',
      });

      const { pages, metadata, title: extractedTitle } = await this.pdfExtractor.extractDocument(filePath);
      console.log(`üîç [INDEXER] Step 1 complete: ${pages.length} pages extracted`);

      // Use bibliography metadata if provided, otherwise fall back to PDF extraction
      const title = bibliographyMetadata?.title || extractedTitle;

      onProgress?.({
        stage: 'extracting',
        progress: 25,
        message: `${pages.length} pages extraites`,
        totalPages: pages.length,
      });

      // 2. Use bibliography metadata for author/year if provided, otherwise extract from PDF
      let author: string | undefined;
      let year: string | undefined;

      if (bibliographyMetadata?.author) {
        author = bibliographyMetadata.author;
        console.log(`   Using bibliography author: ${author}`);
      } else {
        author = await this.pdfExtractor.extractAuthor(filePath);
      }

      if (bibliographyMetadata?.year) {
        year = bibliographyMetadata.year;
        console.log(`   Using bibliography year: ${year}`);
      } else {
        year = await this.pdfExtractor.extractYear(filePath);
      }

      // 3. Extraire le texte complet pour analyse
      const fullText = pages.map((p) => p.text).join('\n\n');

      // 4. D√©tecter la langue du document
      onProgress?.({
        stage: 'analyzing',
        progress: 27,
        message: 'Analyse du document...',
      });

      const language = this.citationExtractor.detectLanguage(fullText);
      console.log(`   Langue d√©tect√©e: ${language}`);

      // 5. Extraction des citations
      onProgress?.({
        stage: 'citations',
        progress: 30,
        message: 'Extraction des citations...',
      });

      const citations = this.citationExtractor.extractCitations(fullText, pages);
      console.log(`   Citations extraites: ${citations.length}`);

      // Statistiques sur les citations
      if (citations.length > 0) {
        const stats = this.citationExtractor.getCitationStatistics(citations);
        console.log(
          `   - ${stats.totalCitations} citations, ${stats.uniqueAuthors} auteurs, ann√©es ${stats.yearRange.min}-${stats.yearRange.max}`
        );
      }

      // 6. G√©n√©ration du r√©sum√© (optionnel)
      let summary: string | undefined;
      let summaryEmbedding: Float32Array | undefined;

      if (this.documentSummarizer) {
        onProgress?.({
          stage: 'summarizing',
          progress: 33,
          message: `G√©n√©ration du r√©sum√© (${this.summarizerConfig.method})...`,
        });

        summary = await this.documentSummarizer.generateSummary(fullText, metadata);

        // G√©n√©rer l'embedding du r√©sum√©
        if (summary) {
          summaryEmbedding = await this.documentSummarizer.generateSummaryEmbedding(summary);
          console.log(`   R√©sum√© g√©n√©r√©: ${summary.split(' ').length} mots`);
        }
      }

      // 7. Cr√©er le document avec donn√©es enrichies
      const documentId = randomUUID();
      const now = new Date();

      const document: PDFDocument = {
        id: documentId,
        fileURL: filePath,
        title,
        author,
        year,
        bibtexKey,
        pageCount: pages.length,
        metadata,
        createdAt: now,
        indexedAt: now,
        lastAccessedAt: now,
        get displayString() {
          if (this.author && this.year) {
            return `${this.author} (${this.year})`;
          }
          return this.title;
        },
      };

      // Ajouter les champs enrichis
      (document as any).language = language;
      (document as any).citationsExtracted = citations;
      (document as any).summary = summary;
      (document as any).summaryEmbedding = summaryEmbedding;

      // 8. Sauvegarder le document
      this.vectorStore.saveDocument(document);

      // 9. Matcher et sauvegarder les citations avec documents existants
      const allDocuments = this.vectorStore.getAllDocuments();
      const citationMatches = this.citationExtractor.matchCitationsWithDocuments(
        citations,
        allDocuments
      );

      // Sauvegarder les citations match√©es en BDD
      for (const citation of citations) {
        const citationId = randomUUID();
        const targetDocId = citationMatches.get(citation.id);

        this.vectorStore.saveCitation({
          id: citationId,
          sourceDocId: documentId,
          targetCitation: citation.text,
          targetDocId,
          context: citation.context,
          pageNumber: citation.pageNumber,
        });
      }

      if (citationMatches.size > 0) {
        console.log(`   Citations match√©es: ${citationMatches.size}/${citations.length}`);
      }

      // 10. Cr√©er les chunks
      onProgress?.({
        stage: 'chunking',
        progress: 40,
        message: 'D√©coupage du texte en chunks...',
      });

      // Pass document metadata to adaptive chunker for context enhancement
      const documentMeta = {
        title: document.title,
        abstract: summary,
      };

      const chunks =
        this.chunker instanceof AdaptiveChunker
          ? this.chunker.createChunks(pages, documentId, documentMeta)
          : this.chunker.createChunks(pages, documentId);

      const stats = this.chunker.getChunkingStats(chunks);
      console.log(
        `üìä Chunking: ${stats.totalChunks} chunks, ${stats.averageWordCount} mots/chunk en moyenne`
      );

      onProgress?.({
        stage: 'chunking',
        progress: 45,
        message: `${chunks.length} chunks cr√©√©s`,
        totalChunks: chunks.length,
      });

      // 11. G√©n√©rer les embeddings et sauvegarder
      onProgress?.({
        stage: 'embedding',
        progress: 50,
        message: 'G√©n√©ration des embeddings...',
        totalChunks: chunks.length,
      });

      // Check if we're using EnhancedVectorStore
      const isEnhancedStore = this.vectorStore instanceof EnhancedVectorStore;
      console.log(`üîç [INDEXER] Step 7: Generating embeddings (EnhancedStore: ${isEnhancedStore})...`);

      if (isEnhancedStore) {
        // Batch processing for EnhancedVectorStore
        console.log('üì¶ Using batch indexing for EnhancedVectorStore');
        const chunksWithEmbeddings = [];

        for (let i = 0; i < chunks.length; i++) {
          const chunk = chunks[i];

          // G√©n√©rer l'embedding
          console.log(`üîç [INDEXER] Generating embedding ${i + 1}/${chunks.length}...`);
          const embedding = await this.ollamaClient.generateEmbedding(chunk.content);
          console.log(`üîç [INDEXER] Embedding ${i + 1} generated, dimension: ${embedding.length}`);

          chunksWithEmbeddings.push({ chunk, embedding });

          // Mise √† jour de la progression
          const progress = 50 + Math.floor((i / chunks.length) * 40);
          onProgress?.({
            stage: 'embedding',
            progress,
            message: `Embeddings: ${i + 1}/${chunks.length}`,
            currentChunk: i + 1,
            totalChunks: chunks.length,
          });

          // Log progression tous les 10 chunks
          if ((i + 1) % 10 === 0 || i === chunks.length - 1) {
            console.log(`  Embeddings: ${i + 1}/${chunks.length}`);
          }
        }

        // Batch add to all indexes (HNSW, BM25, SQLite)
        console.log(`üîç [INDEXER] Step 8: Adding ${chunksWithEmbeddings.length} chunks to indexes...`);
        console.log(`üîç [INDEXER] First embedding dimension: ${chunksWithEmbeddings[0]?.embedding?.length || 'N/A'}`);
        await (this.vectorStore as EnhancedVectorStore).addChunks(chunksWithEmbeddings);
        console.log(`üîç [INDEXER] Step 8 complete: Chunks added to HNSW, BM25, SQLite`);

        onProgress?.({
          stage: 'embedding',
          progress: 95,
          message: 'Index HNSW et BM25 construits',
        });
      } else {
        // Original behavior for VectorStore
        for (let i = 0; i < chunks.length; i++) {
          const chunk = chunks[i];

          // G√©n√©rer l'embedding
          const embedding = await this.ollamaClient.generateEmbedding(chunk.content);

          // Sauvegarder le chunk avec son embedding
          this.vectorStore.saveChunk(chunk, embedding);

          // Mise √† jour de la progression
          const progress = 50 + Math.floor((i / chunks.length) * 45);
          onProgress?.({
            stage: 'embedding',
            progress,
            message: `Embeddings: ${i + 1}/${chunks.length}`,
            currentChunk: i + 1,
            totalChunks: chunks.length,
          });

          // Log progression tous les 10 chunks
          if ((i + 1) % 10 === 0 || i === chunks.length - 1) {
            console.log(`  Embeddings: ${i + 1}/${chunks.length}`);
          }
        }
      }

      // 12. Calculer les similarit√©s avec les autres documents
      onProgress?.({
        stage: 'similarities',
        progress: 95,
        message: 'Calcul des similarit√©s avec les autres documents...',
      });

      // Get the base VectorStore for similarity calculations
      const baseStore = isEnhancedStore
        ? (this.vectorStore as EnhancedVectorStore).getBaseStore()
        : (this.vectorStore as VectorStore);

      const similaritiesCount = baseStore.computeAndSaveSimilarities(
        documentId,
        0.5 // Seuil de similarit√©
      );

      onProgress?.({
        stage: 'completed',
        progress: 100,
        message: `‚úÖ Indexation termin√©e: ${chunks.length} chunks, ${similaritiesCount} similarit√©s`,
      });

      console.log(`‚úÖ PDF index√©: ${document.title}`);
      console.log(`   - ${chunks.length} chunks`);
      console.log(`   - ${stats.totalWords} mots total`);
      console.log(`   - Moyenne: ${stats.averageWordCount} mots/chunk`);
      console.log(`   - ${similaritiesCount} similarit√©s calcul√©es`);

      return document;
    } catch (error) {
      console.error('‚ùå Erreur indexation PDF:', error);
      onProgress?.({
        stage: 'error',
        progress: 0,
        message: `Erreur: ${error}`,
      });
      throw error;
    }
  }

  /**
   * Indexe plusieurs PDFs en batch
   */
  async indexMultiplePDFs(
    filePaths: string[],
    onProgress?: (fileIndex: number, progress: IndexingProgress) => void
  ): Promise<PDFDocument[]> {
    const documents: PDFDocument[] = [];

    for (let i = 0; i < filePaths.length; i++) {
      const filePath = filePaths[i];

      console.log(`\nüìÅ Indexation ${i + 1}/${filePaths.length}: ${filePath}`);

      try {
        const document = await this.indexPDF(filePath, undefined, (progress) => {
          onProgress?.(i, progress);
        });

        documents.push(document);
      } catch (error) {
        console.error(`‚ùå Erreur avec ${filePath}:`, error);
        // Continuer avec les autres fichiers
      }
    }

    console.log(`\n‚úÖ Indexation batch termin√©e: ${documents.length}/${filePaths.length} PDFs`);

    return documents;
  }

  /**
   * R√©-indexe un document existant
   */
  async reindexPDF(documentId: string): Promise<void> {
    // R√©cup√©rer le document
    const document = this.vectorStore.getDocument(documentId);
    if (!document) {
      throw new Error(`Document introuvable: ${documentId}`);
    }

    console.log(`üîÑ R√©-indexation: ${document.title}`);

    // Supprimer l'ancien (les chunks seront supprim√©s en CASCADE)
    this.vectorStore.deleteDocument(documentId);

    // R√©-indexer
    await this.indexPDF(document.fileURL, document.bibtexKey);
  }

  /**
   * V√©rifie si Ollama est disponible
   */
  async checkOllamaAvailability(): Promise<boolean> {
    return await this.ollamaClient.isAvailable();
  }

  /**
   * Liste les mod√®les disponibles
   */
  async listAvailableModels() {
    return await this.ollamaClient.listAvailableModels();
  }

  /**
   * Obtient les statistiques de la base vectorielle
   */
  getStatistics() {
    return this.vectorStore.getStatistics();
  }

  /**
   * Nettoie les chunks orphelins
   */
  cleanOrphanedChunks() {
    return this.vectorStore.cleanOrphanedChunks();
  }

  /**
   * V√©rifie l'int√©grit√© de la base
   */
  verifyIntegrity() {
    return this.vectorStore.verifyIntegrity();
  }
}
