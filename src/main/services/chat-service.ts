// @ts-nocheck
import { LRUCache } from 'lru-cache';
import { pdfService } from './pdf-service.js';
import { BrowserWindow } from 'electron';
import { historyService } from './history-service.js';
import { ContextCompressor } from '../../../backend/core/rag/ContextCompressor.js';
import { getSystemPrompt } from '../../../backend/core/llm/SystemPrompts.js';

// Options enrichies pour le RAG
interface EnrichedRAGOptions {
  context?: boolean;              // Activer le RAG
  useGraphContext?: boolean;      // Utiliser le graphe de connaissances
  includeSummaries?: boolean;     // Utiliser r√©sum√©s au lieu de chunks
  topK?: number;                  // Nombre de r√©sultats de recherche
  additionalGraphDocs?: number;   // Nombre de documents li√©s √† inclure
  window?: BrowserWindow;         // Fen√™tre pour streaming

  // Source type selection (primary = Tropy archives, secondary = PDFs, both = all)
  sourceType?: 'secondary' | 'primary' | 'both';

  // Collection filtering (filter RAG search by Zotero collections)
  collectionKeys?: string[];      // Zotero collection keys to filter by

  // Provider selection
  provider?: 'ollama' | 'embedded' | 'auto';  // LLM provider to use

  // Per-query parameters
  model?: string;                 // Override chat model
  timeout?: number;               // Timeout in milliseconds
  numCtx?: number;                // Context window size in tokens (Ollama num_ctx)
  temperature?: number;           // LLM temperature
  top_p?: number;                 // LLM top_p
  top_k?: number;                 // LLM top_k
  repeat_penalty?: number;        // LLM repeat penalty

  // System prompt configuration (Phase 2.3)
  systemPromptLanguage?: 'fr' | 'en';    // Language for default prompt
  useCustomSystemPrompt?: boolean;       // Use custom prompt
  customSystemPrompt?: string;           // Custom system prompt text
}

// Fonction utilitaire pour hasher une cha√Æne (identifier les questions identiques)
function hashString(str: string): string {
  let hash = 0;
  const normalized = str.toLowerCase().trim();
  for (let i = 0; i < normalized.length; i++) {
    hash = ((hash << 5) - hash) + normalized.charCodeAt(i);
    hash |= 0; // Convert to 32bit integer
  }
  return hash.toString(16);
}

class ChatService {
  private currentStream: any = null;
  private compressor: ContextCompressor = new ContextCompressor();

  // LRU Cache for RAG search results (cache identical queries)
  // üöÄ OPTIMIZED: Increased capacity (100‚Üí200) and TTL (10‚Üí30 minutes)
  private ragCache = new LRUCache<string, any[]>({
    max: 200, // Store up to 200 different queries
    ttl: 1000 * 60 * 30, // 30 minutes TTL
    updateAgeOnGet: true, // Refresh TTL on access
  });

  /**
   * Convertit les r√©sultats de recherche en utilisant les r√©sum√©s au lieu des chunks
   * Si les r√©sum√©s ne sont pas disponibles, retourne les chunks originaux
   */
  private convertChunksToSummaries(searchResults: any[]): any[] {
    const summaryResults: any[] = [];
    const seenDocuments = new Set<string>();
    let summariesFound = 0;

    for (const result of searchResults) {
      const docId = result.document.id;

      // √âviter les doublons (un r√©sum√© par document)
      if (seenDocuments.has(docId)) {
        continue;
      }

      if (result.document.summary) {
        seenDocuments.add(docId);
        summariesFound++;
        summaryResults.push({
          document: result.document,
          chunk: {
            content: result.document.summary,
            pageNumber: 1
          },
          similarity: result.similarity
        });
      }
    }

    // Fallback: if no summaries available, return original chunks
    if (summaryResults.length === 0 && searchResults.length > 0) {
      console.warn('‚ö†Ô∏è  No document summaries found. Falling back to original chunks.');
      console.warn('‚ö†Ô∏è  To use summaries, re-index your documents with summary generation enabled.');
      return searchResults;
    }

    console.log(`üìù Using summaries: ${summariesFound} documents with summaries found`);
    return summaryResults;
  }

  /**
   * R√©cup√®re les documents li√©s via le graphe de connaissances
   */
  private async getRelatedDocumentsFromGraph(
    documentIds: string[],
    limit: number = 3
  ): Promise<Set<string>> {
    const relatedDocs = new Set<string>();
    const vectorStore = pdfService.getVectorStore();

    if (!vectorStore) {
      return relatedDocs;
    }

    for (const docId of documentIds) {
      // R√©cup√©rer documents cit√©s par ce document
      const citedDocs = vectorStore.getDocumentsCitedBy(docId);
      citedDocs.slice(0, Math.ceil(limit / 2)).forEach(id => relatedDocs.add(id));

      // R√©cup√©rer documents qui citent ce document
      const citingDocs = vectorStore.getDocumentsCiting(docId);
      citingDocs.slice(0, Math.ceil(limit / 2)).forEach(id => relatedDocs.add(id));

      // R√©cup√©rer documents similaires
      const similarDocs = vectorStore.getSimilarDocuments(docId, 0.7, limit);
      similarDocs.forEach(({ documentId }) => relatedDocs.add(documentId));
    }

    // Retirer les documents originaux
    documentIds.forEach(id => relatedDocs.delete(id));

    return relatedDocs;
  }

  async sendMessage(
    message: string,
    options: EnrichedRAGOptions = {}
  ): Promise<{ response: string; ragUsed: boolean; sourcesCount: number }> {
    const startTime = Date.now();
    const queryHash = hashString(message);

    try {
      // Obtenir le LLM Provider Manager (g√®re Ollama + mod√®le embarqu√©)
      const llmProviderManager = pdfService.getLLMProviderManager();
      if (!llmProviderManager) {
        throw new Error('LLM Provider Manager not initialized. Load a project first.');
      }

      // Appliquer le provider s√©lectionn√© par l'utilisateur (from RAG settings)
      if (options.provider) {
        console.log(`üîß [CHAT] Setting provider preference: ${options.provider}`);
        llmProviderManager.setProvider(options.provider);
      }

      // V√©rifier qu'au moins un provider est disponible
      const activeProvider = await llmProviderManager.getActiveProvider();
      if (!activeProvider) {
        throw new Error(
          'Aucun LLM disponible.\n\n' +
          'Options:\n' +
          '1. Installez et d√©marrez Ollama (https://ollama.ai)\n' +
          '2. T√©l√©chargez le mod√®le embarqu√© dans Param√®tres ‚Üí LLM'
        );
      }

      console.log(`ü§ñ [CHAT] Using LLM provider: ${llmProviderManager.getActiveProviderName()}`);

      let fullResponse = '';
      let searchResults: any[] = [];
      let relatedDocuments: any[] = [];

      // Si contexte activ√©, rechercher dans les documents
      if (options.context) {
        const searchStart = Date.now();

        // üöÄ FEEDBACK: Send status update - searching
        if (options.window) {
          options.window.webContents.send('chat:status', {
            stage: 'searching',
            message: 'üîç Recherche dans les documents...',
          });
        }

        console.log('üîç [RAG DETAILED DEBUG] Starting RAG search:', {
          query: message.substring(0, 100) + (message.length > 100 ? '...' : ''),
          queryLength: message.length,
          queryHash: queryHash,
          topK: options.topK,
          useGraphContext: options.useGraphContext,
          includeSummaries: options.includeSummaries,
          timestamp: new Date().toISOString(),
        });

        // Check cache first (identical queries = instant results)
        // Include collection filter and source type in cache key to avoid mixing results
        const collectionSuffix = options.collectionKeys?.length ? `-coll:${options.collectionKeys.sort().join(',')}` : '';
        const sourceTypeSuffix = options.sourceType ? `-src:${options.sourceType}` : '-src:both';
        const cacheKey = `${queryHash}-${options.topK || 5}${collectionSuffix}${sourceTypeSuffix}`;
        const cachedResults = this.ragCache.get(cacheKey);

        if (cachedResults) {
          console.log(`üíæ Cache HIT for query hash ${queryHash} (saved ${Date.now() - searchStart}ms)`);
          searchResults = cachedResults;
        } else {
          console.log(`üîç Cache MISS for query hash ${queryHash}, performing search...`);
          searchResults = await pdfService.search(message, {
            topK: options.topK,
            collectionKeys: options.collectionKeys,
            sourceType: options.sourceType,
          });

          // Store in cache for future identical queries
          this.ragCache.set(cacheKey, searchResults);
          console.log(`üíæ Cached ${searchResults.length} results for query hash ${queryHash}`);
        }
        const searchDuration = Date.now() - searchStart;

        // Filter out results with null documents (orphaned chunks)
        searchResults = searchResults.filter(r => r.document !== null);

        console.log('üîç [RAG DETAILED DEBUG] Search completed:', {
          queryHash: queryHash,
          resultsCount: searchResults.length,
          searchDuration: `${searchDuration}ms`,
          topSimilarities: searchResults.slice(0, 5).map(r => r.similarity.toFixed(4)),
          chunkIds: searchResults.slice(0, 3).map(r => r.chunk.id),
          documentTitles: searchResults.slice(0, 3).map(r => r.document?.title || 'Unknown'),
        });

        if (searchResults.length > 0) {
          console.log(`üìö Using ${searchResults.length} context chunks for RAG`);

          // üöÄ FEEDBACK: Send status update - found sources
          if (options.window) {
            options.window.webContents.send('chat:status', {
              stage: 'found',
              message: `üìö ${searchResults.length} sources trouv√©es`,
            });
          }

          // Log first result for debugging
          console.log('üîç [RAG DEBUG] First result:', {
            document: searchResults[0].document?.title || 'Unknown',
            similarity: searchResults[0].similarity,
            chunkLength: searchResults[0].chunk.content.length
          });

          // Si graphe activ√©, r√©cup√©rer documents li√©s
          if (options.useGraphContext) {
            const uniqueDocIds = [...new Set(searchResults.map(r => r.document.id))];
            const relatedDocIds = await this.getRelatedDocumentsFromGraph(
              uniqueDocIds,
              options.additionalGraphDocs || 3
            );

            console.log(`üîó Found ${relatedDocIds.size} related documents via graph`);

            // R√©cup√©rer les documents complets
            const vectorStore = pdfService.getVectorStore();
            if (vectorStore && relatedDocIds.size > 0) {
              relatedDocuments = Array.from(relatedDocIds)
                .map(id => vectorStore.getDocument(id))
                .filter(doc => doc !== null);
            }
          }

          // Si r√©sum√©s activ√©s, utiliser r√©sum√©s au lieu de chunks
          if (options.includeSummaries) {
            console.log('üìù Using document summaries instead of chunks');
            // Remplacer chunks par r√©sum√©s
            searchResults = this.convertChunksToSummaries(searchResults);
            if (relatedDocuments.length > 0) {
              // Ajouter r√©sum√©s des documents li√©s
              relatedDocuments.forEach(doc => {
                if (doc.summary) {
                  searchResults.push({
                    document: doc,
                    chunk: { content: doc.summary, pageNumber: 1 },
                    similarity: 0.7, // Score arbitraire pour documents li√©s
                    isRelatedDoc: true
                  });
                }
              });
            }
          }
        }
      }

      // Apply intelligent compression to context chunks
      if (searchResults.length > 0) {
        const preCompressionSize = searchResults.reduce((sum, r) => sum + r.chunk.content.length, 0);
        console.log(`üóúÔ∏è  [COMPRESSION] Pre-compression context size: ${preCompressionSize} chars (${searchResults.length} chunks)`);

        // Convert search results to compressor format
        const chunksForCompression = searchResults.map(r => ({
          content: r.chunk.content,
          documentId: r.document.id,
          documentTitle: r.document.title,
          pageNumber: r.chunk.pageNumber,
          similarity: r.similarity,
        }));

        // Compress with 20k char target
        const compressionResult = this.compressor.compress(chunksForCompression, message, 20000);

        // Convert back to search result format
        searchResults = compressionResult.chunks.map(chunk => ({
          document: {
            id: chunk.documentId,
            title: chunk.documentTitle,
          },
          chunk: {
            content: chunk.content,
            pageNumber: chunk.pageNumber,
          },
          similarity: chunk.similarity,
        }));

        console.log(`‚úÖ [COMPRESSION] Final stats:`, {
          strategy: compressionResult.stats.strategy,
          originalChunks: compressionResult.stats.originalChunks,
          compressedChunks: compressionResult.stats.compressedChunks,
          originalSize: compressionResult.stats.originalSize,
          compressedSize: compressionResult.stats.compressedSize,
          reduction: `${compressionResult.stats.reductionPercent.toFixed(1)}%`,
        });
      }

      // R√©cup√©rer le contexte du projet
      const projectContext = pdfService.getProjectContext();

      // Build system prompt based on configuration (Phase 2.3)
      const systemPromptLanguage = options.systemPromptLanguage || 'fr';
      const useCustomPrompt = options.useCustomSystemPrompt || false;
      const customPrompt = options.customSystemPrompt;
      const systemPrompt = getSystemPrompt(systemPromptLanguage, useCustomPrompt, customPrompt);

      console.log('ü§ñ [SYSTEM PROMPT] Configuration:', {
        language: systemPromptLanguage,
        useCustom: useCustomPrompt,
        hasCustom: !!customPrompt,
        promptPreview: systemPrompt.substring(0, 100) + '...',
      });

      // Build generation options (commun aux deux cas)
      const generationOptions = {
        temperature: options.temperature,
        top_p: options.top_p,
        top_k: options.top_k,
        repeat_penalty: options.repeat_penalty,
        num_ctx: options.numCtx,  // Context window size for Ollama
      };

      // üöÄ FEEDBACK: Send status update - generating
      if (options.window) {
        options.window.webContents.send('chat:status', {
          stage: 'generating',
          message: '‚ú® G√©n√©ration de la r√©ponse...',
        });
      }

      // Stream la r√©ponse avec contexte RAG si disponible
      if (searchResults.length > 0) {
        console.log('‚úÖ [RAG DETAILED DEBUG] Generating response WITH context:', {
          queryHash: queryHash,
          contextsUsed: searchResults.length,
          avgSimilarity: (searchResults.reduce((sum, r) => sum + r.similarity, 0) / searchResults.length).toFixed(4),
          mode: 'RAG_WITH_SOURCES',
          projectContextLoaded: !!projectContext,
          provider: llmProviderManager.getActiveProviderName(),
          timeout: options.timeout || 600000,
        });

        // Utiliser LLMProviderManager pour la g√©n√©ration (Ollama ou embarqu√©)
        const generator = llmProviderManager.generateWithSources(
          message,
          searchResults,
          projectContext,
          {
            model: options.model,
            timeout: options.timeout,
            generationOptions,
            systemPrompt,
          }
        );
        this.currentStream = generator;

        for await (const chunk of generator) {
          fullResponse += chunk;
          // Envoyer le chunk au renderer si une fen√™tre est fournie
          if (options.window) {
            options.window.webContents.send('chat:stream', chunk);
          }
        }
      } else {
        console.warn('‚ö†Ô∏è  [RAG DETAILED DEBUG] No search results - generating response WITHOUT context');
        console.warn('‚ö†Ô∏è  [RAG DETAILED DEBUG] Fallback mode details:', {
          queryHash: queryHash,
          query: message.substring(0, 100),
          contextRequested: options.context,
          topK: options.topK,
          mode: 'FALLBACK_NO_CONTEXT',
          warning: 'This response will be GENERIC and NOT based on your documents!',
        });

        // Utiliser LLMProviderManager pour la g√©n√©ration sans sources
        const generator = llmProviderManager.generateWithoutSources(
          message,
          [],
          {
            model: options.model,
            timeout: options.timeout,
            generationOptions,
            systemPrompt,
          }
        );
        this.currentStream = generator;

        for await (const chunk of generator) {
          fullResponse += chunk;
          // Envoyer le chunk au renderer si une fen√™tre est fournie
          if (options.window) {
            options.window.webContents.send('chat:stream', chunk);
          }
        }
      }

      const totalDuration = Date.now() - startTime;

      console.log('‚úÖ [RAG DETAILED DEBUG] Chat response completed:', {
        queryHash: queryHash,
        responseLength: fullResponse.length,
        totalDuration: `${totalDuration}ms`,
        ragUsed: searchResults.length > 0,
        timestamp: new Date().toISOString(),
      });

      // Log chat messages and AI operation to history
      const hm = historyService.getHistoryManager();
      if (hm) {
        // Build query params for history
        const queryParams = {
          model: options.model || llmProviderManager.getActiveProviderName(),
          topK: options.topK,
          timeout: options.timeout || 600000,
          temperature: options.temperature,
          top_p: options.top_p,
          top_k: options.top_k,
          repeat_penalty: options.repeat_penalty,
          useGraphContext: options.useGraphContext || false,
          includeSummaries: options.includeSummaries || false,
        };

        // Log user message with query params
        hm.logChatMessage({
          role: 'user',
          content: message,
          queryParams,
        });

        // Log assistant response with sources
        const sources =
          searchResults.length > 0
            ? searchResults.map((r) => ({
                documentId: r.document?.id || '',
                documentTitle: r.document?.title || 'Unknown',
                author: r.document?.author || '',
                year: r.document?.year || 0,
                pageNumber: r.chunk.pageNumber,
                similarity: r.similarity,
                isRelatedDoc: r.isRelatedDoc || false,
              }))
            : undefined;

        hm.logChatMessage({
          role: 'assistant',
          content: fullResponse,
          sources,
          queryParams,
        });

        // Log RAG operation if context was used
        if (options.context && searchResults.length > 0) {
          hm.logAIOperation({
            operationType: 'rag_query',
            durationMs: totalDuration,
            inputText: message,
            inputMetadata: {
              topK: options.topK,
              useGraphContext: options.useGraphContext || false,
              includeSummaries: options.includeSummaries || false,
              sourcesFound: searchResults.length,
              relatedDocumentsFound: relatedDocuments.length,
            },
            modelName: llmProviderManager.getActiveProviderName(),
            modelParameters: {
              temperature: options.temperature || 0.1,
              provider: activeProvider,
            },
            outputText: fullResponse,
            outputMetadata: {
              sources: sources || [],
              responseLength: fullResponse.length,
            },
            success: true,
          });

          console.log(
            `üìù Logged RAG query: ${searchResults.length} sources, ${totalDuration}ms`
          );
        }
      }

      return {
        response: fullResponse,
        ragUsed: searchResults.length > 0,
        sourcesCount: searchResults.length,
      };
    } catch (error) {
      console.error('‚ùå [RAG DETAILED DEBUG] Chat error:', {
        queryHash: queryHash,
        error: error.message,
        stack: error.stack,
      });
      throw error;
    }
  }

  cancelCurrentStream() {
    if (this.currentStream) {
      // TODO: Impl√©menter cancel dans OllamaClient si n√©cessaire
      this.currentStream = null;
      console.log('‚ö†Ô∏è  Chat stream cancelled');
    }
  }
}

export const chatService = new ChatService();
